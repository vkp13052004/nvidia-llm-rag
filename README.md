# NVIDIA LLM with LangServe on GKE

This project deploys NVIDIA LLMs (LLaMA3-8B) using LangChain, LangServe, and Gradio frontend â€” all containerized and ready for GKE.

##  Endpoints
- `/basic_chat`: Generic chat interface
- `/retriever`: RAG pipeline with FAISS
- `/generator`: Prompt-based response

##  Run Locally

```bash
cd backend
uvicorn langserve_app:app --reload
```

##  Docker Build

```bash
docker build -t your-dockerhub/nvidia-llm .
docker push your-dockerhub/nvidia-llm
```

##  Deploy on GKE

```bash
kubectl apply -f deployment/k8s_deployment.yaml
```

## Gradio Frontend

```bash
cd frontend
python gradio_app.py
```
