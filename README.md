# NVIDIA LLM with LangServe on GKE

This project deploys NVIDIA LLMs (LLaMA3-8B) using LangChain, LangServe, and Gradio frontend â€” all containerized and ready for GKE.

## ğŸ”Œ Endpoints
- `/basic_chat`: Generic chat interface
- `/retriever`: RAG pipeline with FAISS
- `/generator`: Prompt-based response

## ğŸš€ Run Locally

```bash
cd backend
uvicorn langserve_app:app --reload
```

## ğŸ³ Docker Build

```bash
docker build -t your-dockerhub/nvidia-llm .
docker push your-dockerhub/nvidia-llm
```

## â˜ï¸ Deploy on GKE

```bash
kubectl apply -f deployment/k8s_deployment.yaml
```

## ğŸ–¼ï¸ Gradio Frontend

```bash
cd frontend
python gradio_app.py
```
